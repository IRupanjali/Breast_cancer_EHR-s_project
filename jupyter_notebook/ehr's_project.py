# -*- coding: utf-8 -*-
"""EHR's project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KaLnixmditoLlvM1sBSsn2zd0BRducPN
"""

import pandas as pd

china_df = pd.read_excel('/content/China.xlsx')
korea_df = pd.read_excel('/content/Korea.xlsx')

china_sheets = pd.read_excel('/content/China.xlsx', sheet_name=None)
korea_sheets = pd.read_excel('/content/Korea.xlsx', sheet_name=None)

for sheet_name, df in china_sheets.items():
    print(f"--- Info for sheet: {sheet_name} (China.xlsx) ---")
    df.info()
    print(f"\n--- Head for sheet: {sheet_name} (China.xlsx) ---")
    display(df.head())
    print("\n" + "="*50 + "\n")

for sheet_name, df in korea_sheets.items():
    print(f"--- Info for sheet: {sheet_name} (Korea.xlsx) ---")
    df.info()
    print(f"\n--- Head for sheet: {sheet_name} (Korea.xlsx) ---")
    display(df.head())
    print("\n" + "="*50 + "\n")

display(china_sheets['Raw data'].describe())

display(korea_sheets['case'].describe())

china_df = china_sheets['Raw data']
korea_df = korea_sheets['case']

"""## Clean column names

### Subtask:
Standardize column names for easier access and manipulation in both `china_df` and `korea_df`.

**Reasoning**:
Iterate through the columns of each dataframe and apply string cleaning methods to standardize the names.
"""

def clean_column_names(df):
    """Cleans column names by removing leading/trailing spaces and replacing spaces with underscores."""
    df.columns = df.columns.str.strip().str.replace(' ', '_')
    return df

china_df = clean_column_names(china_df)
korea_df = clean_column_names(korea_df)

print("Cleaned column names for china_df:")
print(china_df.columns)

print("\nCleaned column names for korea_df:")
print(korea_df.columns)

print("Shape of china_df:", china_df.shape)
print("Shape of korea_df:", korea_df.shape)

print("Missing values in china_df:")
display(china_df.isnull().sum())

print("\nMissing values in korea_df:")
display(korea_df.isnull().sum())

"""## Handle missing values

### Subtask:
Impute missing values in `china_df` and `korea_df`.

**Reasoning**:
Impute missing numerical values with the median and missing object type values with the mode.
"""

# Impute missing values in china_df
for column in china_df.columns:
    if china_df[column].isnull().any():
        if china_df[column].dtype in ['int64', 'float64']:
            median_val = china_df[column].median()
            china_df[column].fillna(median_val, inplace=True)
            print(f"Imputed missing values in '{column}' with median: {median_val}")
        elif china_df[column].dtype == 'object':
            mode_val = china_df[column].mode()[0]
            china_df[column].fillna(mode_val, inplace=True)
            print(f"Imputed missing values in '{column}' with mode: {mode_val}")

# Impute missing values in korea_df
for column in korea_df.columns:
    if korea_df[column].isnull().any():
        if korea_df[column].dtype in ['int64', 'float64']:
            median_val = korea_df[column].median()
            korea_df[column].fillna(median_val, inplace=True)
            print(f"Imputed missing values in '{column}' with median: {median_val}")
        elif korea_df[column].dtype == 'object':
            # Handle cases where mode might be empty (all values are NaN)
            if not korea_df[column].mode().empty:
                mode_val = korea_df[column].mode()[0]
                korea_df[column].fillna(mode_val, inplace=True)
                print(f"Imputed missing values in '{column}' with mode: {mode_val}")
            else:
                print(f"Column '{column}' has only missing values, cannot impute with mode.")


print("\nMissing values after imputation in china_df:")
display(china_df.isnull().sum())

print("\nMissing values after imputation in korea_df:")
display(korea_df.isnull().sum())

"""## Convert data types

### Subtask:
Ensure columns in both dataframes have appropriate data types.

**Reasoning**:
Inspect the current data types and convert columns to appropriate types (e.g., numeric, datetime) based on their content.
"""

print("Data types of china_df before conversion:")
print(china_df.dtypes)

print("\nData types of korea_df before conversion:")
print(korea_df.dtypes)

# Convert relevant columns to appropriate types
# Based on the initial data info (cell 39e511ea),
# 'OP_date', 'Recur_date', 'DM_date', 'death_date', 'Last_F/U' in china_df should be datetime
# Some object columns in korea_df might need encoding or mapping later

date_columns_china = ['OP_date', 'Recur_date', 'DM_date', 'death_date', 'Last_F/U']
for col in date_columns_china:
    if col in china_df.columns:
        china_df[col] = pd.to_datetime(china_df[col], errors='coerce')

# Check data types after conversion
print("\nData types of china_df after conversion:")
print(china_df.dtypes)

print("\nData types of korea_df after conversion:")
print(korea_df.dtypes)

"""## Handle categorical data

### Subtask:
Identify and inspect categorical columns in both dataframes.

**Reasoning**:
Identify object type columns and display their unique values to determine the appropriate encoding strategy.
"""

# Identify object type columns
object_columns_china = china_df.select_dtypes(include='object').columns
object_columns_korea = korea_df.select_dtypes(include='object').columns

print("Object columns in china_df:", object_columns_china)
print("Object columns in korea_df:", object_columns_korea)

# Display unique values for object columns
print("\nUnique values in object columns of china_df:")
for col in object_columns_china:
    print(f"- {col}: {china_df[col].unique()[:10]}...") # Displaying up to 10 unique values

print("\nUnique values in object columns of korea_df:")
for col in object_columns_korea:
    print(f"- {col}: {korea_df[col].unique()[:10]}...") # Displaying up to 10 unique values

"""### Subtask:
Handle mixed-type categorical columns ('B_G' and 'N_G') in `china_df`.

**Reasoning**:
Map the 'UA' value to a numerical representation (0) and retain the existing numerical values to convert the columns to a consistent numerical type.
"""

# Handle 'B_G' and 'N_G' in china_df
# Map 'UA' to 0 and keep numerical values
ua_mapping = {'UA': 0}

if 'B_G' in china_df.columns:
    # Ensure the column is string type before mapping 'UA'
    china_df['B_G'] = china_df['B_G'].astype(str).map(ua_mapping).fillna(china_df['B_G']) # Map 'UA' and keep numerical
    china_df['B_G'] = china_df['B_G'].astype(int) # Convert to integer after mapping

if 'N_G' in china_df.columns:
     # Ensure the column is string type before mapping 'UA'
    china_df['N_G'] = china_df['N_G'].astype(str).map(ua_mapping).fillna(china_df['N_G']) # Map 'UA' and keep numerical
    china_df['N_G'] = china_df['N_G'].astype(int) # Convert to integer after mapping


print("\nUnique values and dtypes after handling 'B_G' and 'N_G' in china_df:")
mixed_cols_china = ['B_G', 'N_G']
for col in mixed_cols_china:
     if col in china_df.columns:
        print(f"- {col}: {china_df[col].unique()} (dtype: {china_df[col].dtype})")

"""### Subtask:
Encode the ordinal 'T-Nstage' column in `china_df`.

**Reasoning**:
Map the ordinal 'T-Nstage' column to numerical representations that preserve its inherent order.
"""

# Map ordinal 'T-Nstage' in china_df
# Based on unique values ['2A' '1A' '2B' '3C' '3A' '3B' 0]
# Map them based on perceived progression, and map '0' to 0.
tnstage_mapping_robust = {
    '1a': 1, '2a': 2, '2b': 3, '3c': 4, '3a': 5, '3b': 6,
    '0': 0 # Map string '0' and integer 0
}

if 'T-Nstage' in china_df.columns:
    # Handle integer 0 explicitly first
    china_df['T-Nstage'] = china_df['T-Nstage'].replace(0, 0) # Ensure integer 0 is mapped to 0
    # Now map string values (lowercase and strip)
    china_df['T-Nstage'] = china_df['T-Nstage'].astype(str).str.lower().str.strip().map(tnstage_mapping_robust).fillna(china_df['T-Nstage']) # Map strings, keep original numerical 0 if not mapped
    # Map any remaining NaNs (from original strings not in mapping or potential NaNs) to -1
    china_df['T-Nstage'] = china_df['T-Nstage'].fillna(-1)
    china_df['T-Nstage'] = china_df['T-Nstage'].astype(int) # Convert to integer


print("\nUnique values and dtype after encoding 'T-Nstage' in china_df:")
if 'T-Nstage' in china_df.columns:
     print(f"- T-Nstage: {china_df['T-Nstage'].unique()} (dtype: {china_df['T-Nstage'].dtype})")

"""### Subtask:
Drop columns in `korea_df` that contain only NA values.

**Reasoning**:
The specified columns ('Group', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'ER', 'PR') contain only NA values and cannot be used for analysis in their current state. Dropping them is a way to handle them.

### Subtask:
Replace NA values with 0 in specific `korea_df` columns ('Group', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'ER', 'PR').

**Reasoning**:
Replace the persistent NA values in the specified columns with a constant value (0) as requested by the user.
"""

columns_to_fill_with_zero = ['Group', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'ER', 'PR']

for col in columns_to_fill_with_zero:
    if col in korea_df.columns:
        korea_df[col].fillna(0, inplace=True)
        print(f"Filled NA values in '{col}' with 0.")

print("\nMissing values in korea_df after filling specified columns with 0:")
display(korea_df.isnull().sum())

print("\nUnique values and dtypes in specified columns after filling with 0:")
for col in columns_to_fill_with_zero:
    if col in korea_df.columns:
        print(f"- {col}: {korea_df[col].unique()} (dtype: {korea_df[col].dtype})")

"""## Normalize numerical data (Optional)

### Subtask:
Normalize numerical columns in both dataframes.

**Reasoning**:
Apply Min-Max scaling to numerical columns to bring them to a similar range, which can be beneficial for certain analysis techniques.
"""

from sklearn.preprocessing import MinMaxScaler

# Identify numerical columns to normalize
# Exclude columns that are already binary (0/1) or represent categories after encoding
numerical_cols_china = china_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Exclude columns that are clearly IDs or already encoded categorical (like B_G, N_G, T-Nstage, which are now int)
# Let's exclude 'Numbering' as it's an ID. B_G, N_G, T-Nstage were treated as ordinal/categorical.
numerical_cols_china_to_normalize = [col for col in numerical_cols_china if col not in ['Numbering', 'B_G', 'N_G', 'T-Nstage']]


numerical_cols_korea = korea_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Exclude columns that are clearly IDs, or already encoded categorical (like TNMstage, Histopathological_...)
# Also exclude the binary columns that we just filled with 0s if they are not intended for scaling
# Let's exclude 'TNMstage' (mapped to -1) and Histopathological one-hot encoded columns
# Also exclude the columns we just filled with 0: 'Group', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'ER', 'PR'
columns_filled_with_zero = ['Group', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'ER', 'PR']
numerical_cols_korea_to_normalize = [col for col in numerical_cols_korea if col not in ['TNMstage'] and not col.startswith('Histopathological_') and col not in columns_filled_with_zero]


print("Numerical columns in china_df to normalize:", numerical_cols_china_to_normalize)
print("Numerical columns in korea_df to normalize:", numerical_cols_korea_to_normalize)


# Apply Min-Max Scaling
scaler = MinMaxScaler()

if numerical_cols_china_to_normalize:
    china_df[numerical_cols_china_to_normalize] = scaler.fit_transform(china_df[numerical_cols_china_to_normalize])
    print("\nNormalized numerical columns in china_df.")

if numerical_cols_korea_to_normalize:
    korea_df[numerical_cols_korea_to_normalize] = scaler.fit_transform(korea_df[numerical_cols_korea_to_normalize])
    print("Normalized numerical columns in korea_df.")


print("\nFirst 5 rows of china_df after normalization:")
display(china_df.head())

print("\nFirst 5 rows of korea_df after normalization:")
display(korea_df.head())

print("Columns in korea_df:")
print(korea_df.columns)

"""## Save processed data

### Subtask:
Save the cleaned, encoded, and normalized dataframes to files.

**Reasoning**:
Save the processed dataframes to persistent files (e.g., CSV) for future analysis or use.
"""

# Save processed dataframes to CSV files
china_df.to_csv('china_processed.csv', index=False)
korea_df.to_csv('korea_processed.csv', index=False)

print("Processed data saved to 'china_processed.csv' and 'korea_processed.csv'")

"""# Task
Perform exploratory data analysis on the processed breast cancer EHR datasets ("china_processed.csv" and "korea_processed.csv") to understand feature distributions, relationships, and potential subgroups, in preparation for clustering analysis.

## Load processed data

### Subtask:
Load the preprocessed dataframes (`china_processed.csv` and `korea_processed.csv`) back into the environment.

**Reasoning**:
Load the processed CSV files into pandas DataFrames.
"""

china_processed_df = pd.read_csv('china_processed.csv')
korea_processed_df = pd.read_csv('korea_processed.csv')

print("First 5 rows of china_processed_df:")
display(china_processed_df.head())

print("\nFirst 5 rows of korea_processed_df:")
display(korea_processed_df.head())

"""## Inspect data

### Subtask:
Display the first few rows (`head()`), check data types (`dtypes`), and get descriptive statistics (`describe()`) for the loaded dataframes to re-familiarize with the data.

**Reasoning**:
Display the first few rows, data types, and descriptive statistics for both dataframes to understand their structure and content.
"""

print("First 5 rows of china_processed_df:")
display(china_processed_df.head())

print("\nData types of china_processed_df:")
print(china_processed_df.dtypes)

print("\nDescriptive statistics for numerical columns in china_processed_df:")
display(china_processed_df.describe())

print("\nFirst 5 rows of korea_processed_df:")
display(korea_processed_df.head())

print("\nData types of korea_processed_df:")
print(korea_processed_df.dtypes)

print("\nDescriptive statistics for numerical columns in korea_processed_df:")
display(korea_processed_df.describe())

"""## Analyze feature distributions

### Subtask:
Visualize the distribution of individual features (numerical and categorical) using histograms, box plots, bar plots, etc., to understand their ranges, central tendencies, and variability.

**Reasoning**:
Visualize the distribution of numerical features in `china_processed_df` using histograms and box plots.
"""

import matplotlib.pyplot as plt
import seaborn as sns

numerical_cols_china = china_processed_df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Remove columns that are likely identifiers or encoded categories
cols_to_exclude_china = ['Numbering', 'Initial_OP', 'age35', 'age40', 'ER1', 'PR1', 'HR1', 'HER2_1', 'TN', 'subtype', 'patho1', 'pT', 'pN', 'stage', 'stage123', 'adj_Chemo', 'chemo', 'adj_RT', 'adj_Hor', 'Hormone', 'LR_site', 'LR_OP', 'LR_ER', 'LR_PR', 'LR_HR', 'LR_HER2', 'LR_TN', 'LR_subtype', 'LR_patho', 'LR_CTx', 'LR_RT', 'LR_HTx', 'distant_meta', 'survival', 'B_G', 'N_G', 'T-Nstage']

numerical_cols_china_viz = [col for col in numerical_cols_china if col not in cols_to_exclude_china]

print("Numerical columns selected for visualization in china_processed_df:")
print(numerical_cols_china_viz)

# Plot histograms
n_cols = 3
n_rows = (len(numerical_cols_china_viz) + n_cols - 1) // n_cols
plt.figure(figsize=(n_cols * 5, n_rows * 4))
for i, col in enumerate(numerical_cols_china_viz):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.histplot(data=china_processed_df, x=col, kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Plot box plots
n_cols = 3
n_rows = (len(numerical_cols_china_viz) + n_cols - 1) // n_cols
plt.figure(figsize=(n_cols * 5, n_rows * 4))
for i, col in enumerate(numerical_cols_china_viz):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.boxplot(data=china_processed_df, y=col)
    plt.title(f'Box plot of {col}')
plt.tight_layout()
plt.show()

"""**Reasoning**:
Visualize the distribution of numerical features in `korea_processed_df` using histograms and box plots.


"""

numerical_cols_korea = korea_processed_df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Exclude columns that are likely identifiers or encoded categories, including those filled with 0
cols_to_exclude_korea = ['TNMstage', 'Group', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'ER', 'PR']
numerical_cols_korea_viz = [col for col in numerical_cols_korea if col not in cols_to_exclude_korea and not col.startswith('Histopathological_')]


print("\nNumerical columns selected for visualization in korea_processed_df:")
print(numerical_cols_korea_viz)

# Plot histograms
n_cols = 3
n_rows = (len(numerical_cols_korea_viz) + n_cols - 1) // n_cols
plt.figure(figsize=(n_cols * 5, n_rows * 4))
for i, col in enumerate(numerical_cols_korea_viz):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.histplot(data=korea_processed_df, x=col, kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Plot box plots
n_cols = 3
n_rows = (len(numerical_cols_korea_viz) + n_cols - 1) // n_cols
plt.figure(figsize=(n_cols * 5, n_rows * 4))
for i, col in enumerate(numerical_cols_korea_viz):
    plt.subplot(n_rows, n_cols, i + 1)
    sns.boxplot(data=korea_processed_df, y=col)
    plt.title(f'Box plot of {col}')
plt.tight_layout()
plt.show()

"""**Reasoning**:
Visualize the distribution of categorical features (object type) in `china_processed_df` and `korea_processed_df` using count plots.


"""

# Identify object type columns
object_columns_china = china_processed_df.select_dtypes(include='object').columns
object_columns_korea = korea_processed_df.select_dtypes(include='object').columns

print("Object columns in china_processed_df:", object_columns_china)
print("Object columns in korea_processed_df:", object_columns_korea)

# Plot count plots for object columns in china_processed_df
if len(object_columns_china) > 0:
    n_cols = 2
    n_rows = (len(object_columns_china) + n_cols - 1) // n_cols
    plt.figure(figsize=(n_cols * 6, n_rows * 5))
    for i, col in enumerate(object_columns_china):
        plt.subplot(n_rows, n_cols, i + 1)
        sns.countplot(data=china_processed_df, x=col)
        plt.title(f'Count of {col}')
        plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
else:
    print("No object columns found in china_processed_df for plotting.")

# Plot count plots for object columns in korea_processed_df
if len(object_columns_korea) > 0:
    n_cols = 2
    n_rows = (len(object_columns_korea) + n_cols - 1) // n_cols
    plt.figure(figsize=(n_cols * 6, n_rows * 5))
    for i, col in enumerate(object_columns_korea):
        plt.subplot(n_rows, n_cols, i + 1)
        sns.countplot(data=korea_processed_df, x=col)
        plt.title(f'Count of {col}')
        plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
else:
    print("No object columns found in korea_processed_df for plotting.")

"""## Explore feature relationships

### Subtask:
Analyze the relationships between pairs of features using scatter plots, correlation matrices, or cross-tabulations to identify potential dependencies or patterns that might be relevant for clustering.

**Reasoning**:
Calculate and visualize the correlation matrix for the selected numerical columns in china_processed_df.
"""

# Calculate correlation matrix for china_processed_df numerical columns
china_corr_matrix = china_processed_df[numerical_cols_china_viz].corr()

# Visualize the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(china_corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Features in China Dataset')
plt.show()

"""**Reasoning**:
Calculate and visualize the correlation matrix for the selected numerical columns in korea_processed_df.


"""

# Calculate correlation matrix for korea_processed_df numerical columns
korea_corr_matrix = korea_processed_df[numerical_cols_korea_viz].corr()

# Visualize the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(korea_corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Features in Korea Dataset')
plt.show()

"""**Reasoning**:
Create scatter plots for interesting pairs of numerical features from both dataframes based on the correlation matrices. Also, explore relationships between categorical features using count plots with hue.


"""

# Scatter plots for interesting correlations in china_processed_df
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.scatterplot(data=china_processed_df, x='age', y='ILRR_age')
plt.title('age vs ILRR_age (China)')

plt.subplot(1, 3, 2)
sns.scatterplot(data=china_processed_df, x='LR_DM_mon', y='LR_OS_mon')
plt.title('LR_DM_mon vs LR_OS_mon (China)')

plt.subplot(1, 3, 3)
sns.scatterplot(data=china_processed_df, x='OP_OS_mon', y='OP_DM_mon')
plt.title('OP_OS_mon vs OP_DM_mon (China)')

plt.tight_layout()
plt.show()


# Scatter plots for interesting correlations in korea_processed_df
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.scatterplot(data=korea_processed_df, x='TCmmolL', y='LDLmmolL')
plt.title('TCmmolL vs LDLmmolL (Korea)')

plt.subplot(1, 3, 2)
sns.scatterplot(data=korea_processed_df, x='RBP4ugml', y='BMIkgm2')
plt.title('RBP4ugml vs BMIkgm2 (Korea)')

plt.subplot(1, 3, 3)
sns.scatterplot(data=korea_processed_df, x='MenarcheAgeyears', y='Age')
plt.title('MenarcheAgeyears vs Age (Korea)')

plt.tight_layout()
plt.show()


# Explore relationships between categorical features in korea_processed_df using count plots with hue
categorical_cols_korea = korea_processed_df.select_dtypes(include='object').columns.tolist()

# Let's pick a few interesting pairs that might relate to outcomes or groups
# For example, 'Group' vs 'Metastasis', 'Group' vs 'ER', 'Group' vs 'PR'
categorical_pairs_korea = [('Group', 'Metastasis'), ('Group', 'ER'), ('Group', 'PR')]

plt.figure(figsize=(15, 5))
for i, (col1, col2) in enumerate(categorical_pairs_korea):
    if col1 in korea_processed_df.columns and col2 in korea_processed_df.columns:
        plt.subplot(1, 3, i + 1)
        sns.countplot(data=korea_processed_df, x=col1, hue=col2)
        plt.title(f'{col1} vs {col2} (Korea)')
        plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

# Cross-tabulation example for categorical features in korea_processed_df
print("\nCross-tabulation of Group and Metastasis (Korea):")
display(pd.crosstab(korea_processed_df['Group'], korea_processed_df['Metastasis']))

print("\nCross-tabulation of Group and ER (Korea):")
display(pd.crosstab(korea_processed_df['Group'], korea_processed_df['ER']))

print("\nCross-tabulation of Group and PR (Korea):")
display(pd.crosstab(korea_processed_df['Group'], korea_processed_df['PR']))

"""## Investigate potential subgroups

### Subtask:
Explore potential patient subgroups based on combinations of clinical features using visualizations.

**Reasoning**:
Select key numerical features and create scatter plots with color-coding to visualize potential patient subgroups based on combinations of these features in both dataframes. Then, generate pair plots for a subset of numerical features to explore pairwise relationships and potential clustering patterns. Finally, use box plots or violin plots to explore the relationship between numerical features and categorical features.
"""

# Select key numerical features for visualization in china_processed_df
# Based on distributions, correlations, and clinical relevance
china_numerical_subset = china_processed_df[['age', 'Tcm', 'FU_duration', 'OP_OS_mon', 'survival']]

# Scatter plots with color-coding (e.g., using 'survival')
plt.figure(figsize=(18, 5))

plt.subplot(1, 3, 1)
sns.scatterplot(data=china_processed_df, x='age', y='Tcm', hue='survival')
plt.title('Age vs Tcm colored by Survival (China)')

plt.subplot(1, 3, 2)
sns.scatterplot(data=china_processed_df, x='FU_duration', y='OP_OS_mon', hue='survival')
plt.title('FU_duration vs OP_OS_mon colored by Survival (China)')

plt.subplot(1, 3, 3)
sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='survival')
plt.title('Age vs OP_OS_mon colored by Survival (China)')

plt.tight_layout()
plt.show()

# Select key numerical features for visualization in korea_processed_df
# Based on distributions, correlations, and clinical relevance
korea_numerical_subset = korea_processed_df[['Age', 'BMIkgm2', 'FBGmmolL', 'TCmmolL', 'Group', 'Metastasis', 'ER', 'PR']]


# Scatter plots with color-coding (e.g., using 'Group', 'Metastasis', 'ER', 'PR')
plt.figure(figsize=(18, 10))

plt.subplot(2, 2, 1)
sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='Group')
plt.title('Age vs BMIkgm2 colored by Group (Korea)')

plt.subplot(2, 2, 2)
sns.scatterplot(data=korea_processed_df, x='FBGmmolL', y='TCmmolL', hue='Metastasis')
plt.title('FBGmmolL vs TCmmolL colored by Metastasis (Korea)')

plt.subplot(2, 2, 3)
sns.scatterplot(data=korea_processed_df, x='Age', y='TCmmolL', hue='ER')
plt.title('Age vs TCmmolL colored by ER (Korea)')

plt.subplot(2, 2, 4)
sns.scatterplot(data=korea_processed_df, x='BMIkgm2', y='FBGmmolL', hue='PR')
plt.title('BMIkgm2 vs FBGmmolL colored by PR (Korea)')

plt.tight_layout()
plt.show()

# Pair plots for a subset of numerical features
print("\nPair plot for a subset of numerical features in china_processed_df:")
sns.pairplot(china_numerical_subset, hue='survival')
plt.suptitle('Pair Plot of China Numerical Subset (colored by Survival)', y=1.02)
plt.show()

print("\nPair plot for a subset of numerical features in korea_processed_df:")
sns.pairplot(korea_numerical_subset, hue='Group')
plt.suptitle('Pair Plot of Korea Numerical Subset (colored by Group)', y=1.02)
plt.show()

# Box plots or violin plots to combine categorical and numerical features
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(data=korea_processed_df, x='Group', y='Age')
plt.title('Age distribution by Group (Korea)')

plt.subplot(1, 3, 2)
sns.violinplot(data=korea_processed_df, x='Metastasis', y='BMIkgm2')
plt.title('BMIkgm2 distribution by Metastasis (Korea)')

plt.subplot(1, 3, 3)
sns.boxplot(data=korea_processed_df, x='ER', y='TCmmolL')
plt.title('TCmmolL distribution by ER (Korea)')

plt.tight_layout()
plt.show()

"""## Summarize eda findings

### Subtask:
Document the key observations and insights gained from the exploratory data analysis, particularly those that might inform feature selection and the choice of clustering algorithm.

**Reasoning**:
Summarize the key observations from the EDA steps, discuss implications for feature selection, and reflect on the choice of clustering algorithm based on the data characteristics.
"""

# Document key observations and insights

print("--- Key Observations and Insights from EDA ---")

print("\nChina Dataset:")
print("- Numerical Feature Distributions: Features like 'age', 'Tcm', and the various time-to-event metrics ('OP_LR_mon', 'LR_DM_mon', 'LR_OS_mon', 'OP_OS_mon', 'OP_DM_mon', 'FU_duration') show varying distributions. Some appear relatively normally distributed, while others might be skewed or show multiple modes, suggesting potential subgroups.")
print("- Feature Relationships: Significant positive correlations were observed between different time-to-event variables (e.g., LR_DM_mon and LR_OS_mon; OP_OS_mon and OP_DM_mon). This indicates that these metrics are related and might capture similar aspects of patient outcomes. 'age' shows some correlation with 'ILRR_age'.")
print("- Potential Subgroups: Scatter plots colored by 'survival' show some visual separation, particularly for longer follow-up durations and overall survival times. Patients with longer survival times tend to cluster in certain areas of the plots. This suggests that survival status is a strong indicator of subgroups.")
print("- Categorical Features: 'B_G', 'N_G', and 'T-Nstage' were handled as ordinal/categorical and show distinct categories. The distribution across these categories might indicate subgroups based on tumor grade and stage.")

print("\nKorea Dataset:")
print("- Numerical Feature Distributions: Features like 'Age', 'MenarcheAgeyears', 'FBGmmolL', 'TGmmolL', 'TCmmolL', 'HDLmmolL', 'LDLmmolL', 'RBP4ugml', and 'BMIkgm2' generally appear unimodal, though some show slight skewness or potential outliers (visible in box plots). The normalization applied during preprocessing brings these features to a similar scale.")
print("- Feature Relationships: Expected correlations were observed among metabolic indicators (e.g., TCmmolL and LDLmmolL). RBP4ugml shows a correlation with BMIkgm2. The relationship between MenarcheAgeyears and Age is weak.")
print("- Potential Subgroups: The most striking observation is the clear separation based on the 'Group' variable (Case vs Control). This is evident in scatter plots and box plots, where 'Control' patients show distinct patterns for outcome-related categorical features (Metastasis, ER, PR - all negative) and potentially different distributions for some numerical features (e.g., Age, metabolic indicators). This strong pre-defined grouping is a key characteristic of this dataset.")
print("- Categorical Features: 'Group', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'Histopathological', 'ER', 'PR', and 'TNMstage' are important categorical features. Cross-tabulations confirmed the strong association between 'Group' and 'Metastasis', 'ER', and 'PR'. 'Histopathological' has many categories and some might be less frequent.")

print("\nImplications for Feature Selection:")
print("- China Dataset: Features related to survival and follow-up duration ('OP_OS_mon', 'FU_duration', 'OP_DM_mon', etc.) are strong candidates for clustering as they appear to differentiate patient outcomes. 'age', 'Tcm', and the encoded categorical features ('B_G', 'N_G', 'T-Nstage') also show potential for defining subgroups based on clinical characteristics. Highly correlated time-to-event variables might be redundant, so selecting a representative subset could be considered.")
print("- Korea Dataset: The 'Group' variable is a primary differentiator and will likely dominate any clustering unless explicitly excluded or handled. For clustering within the 'Case' group, features related to metabolic profile ('FBGmmolL', 'TGmmolL', 'TCmmolL', 'HDLmmolL', 'LDLmmolL', 'RBP4ugml', 'BMIkgm2') and clinical characteristics ('Age', 'Miscarriage', 'Menopause', 'P53', 'Metastasis', 'ER', 'PR', 'TNMstage', 'Histopathological') are relevant. Consideration should be given to the high number of missing values initially in some Korea columns (P53, Metastasis, Histopathological, ER, PR, TNMstage) and how the imputation (filling with 0) might impact clustering, especially if 0 is not a meaningful category.")

print("\nImplications for Choice of Clustering Algorithm:")
print("- China Dataset: Contains a mix of numerical and categorical (ordinal/encoded) features. An algorithm that can handle mixed data types or a two-step approach (e.g., clustering numerical and categorical features separately and then combining) might be necessary. Algorithms like K-Modes (for categorical), K-Means (for numerical), or Gower distance-based methods (for mixed data) could be considered. The visual suggestions of subgroups based on survival might guide the choice towards algorithms that can identify such structures.")
print("- Korea Dataset: Has a strong pre-defined grouping ('Case' vs 'Control'). Clustering the entire dataset might simply recover this known distinction. To identify novel subgroups within the 'Case' population, clustering should likely be performed on a subset of the data containing only 'Case' patients, using relevant clinical and metabolic features. The dataset also contains mixed data types (numerical and categorical). The presence of categorical features with varying numbers of categories and the potential impact of the chosen imputation strategy (filling with 0) should be considered when selecting an algorithm (e.g., K-Means after appropriate encoding, K-Modes, or Gower distance-based methods).")

"""## Summary:

### Data Analysis Key Findings

*   **China Dataset:**
    *   Numerical features like 'age', 'Tcm', and time-to-event metrics exhibit varying distributions; some are skewed or multi-modal, suggesting potential subgroups.
    *   Strong positive correlations exist between different time-to-event variables (e.g., `LR_DM_mon` and `LR_OS_mon`), indicating related outcome aspects.
    *   Scatter plots colored by 'survival' visually suggest subgroups, with longer survival times clustering in specific areas.
    *   Categorical features (`B_G`, `N_G`, `T-Nstage`) show distinct categories that may define subgroups based on tumor grade and stage.
*   **Korea Dataset:**
    *   Numerical features (metabolic indicators, BMI) generally appear unimodal but may have skewness or outliers.
    *   Expected correlations are seen among metabolic indicators (e.g., `TCmmolL` and `LDLmmolL`), and between `RBP4ugml` and `BMIkgm2`.
    *   A clear separation exists based on the 'Group' variable (Case vs Control), evident in visualizations, with 'Control' patients showing no metastasis and negative ER/PR status.
    *   Categorical features like 'Group', 'Metastasis', 'ER', and 'PR' are strongly associated, with cross-tabulations confirming the distinction between Case and Control groups regarding these outcomes.

### Insights or Next Steps

*   For the China dataset, focus feature selection on survival/follow-up duration and encoded clinical characteristics (`age`, `Tcm`, `B_G`, `N_G`, `T-Nstage`), potentially selecting a subset of highly correlated time-to-event variables to avoid redundancy.
*   For the Korea dataset, clustering should likely be performed on the 'Case' group subset to identify novel subgroups within the patient population, utilizing metabolic and clinical features. The impact of the chosen imputation strategy (filling with 0) for categorical features needs careful consideration during algorithm selection, as 0 may not represent a meaningful category.

## Feature Selection

### Subtask:
Choose the relevant clinical features from both dataframes that will be used for clustering.

**Reasoning**:
Select a subset of columns based on domain knowledge or initial data exploration that are most relevant for identifying patient groups through clustering.
"""

from sklearn.cluster import DBSCAN
import numpy as np

# Select numerical features for clustering from china_processed_df
# Exclude object type columns (should be none after preprocessing) and identifier columns
china_features_for_clustering = china_processed_df.select_dtypes(include=np.number).columns.tolist()
china_features_for_clustering = [col for col in china_features_for_clustering if col not in ['Numbering']] # Exclude Numbering as it's an ID

# Select numerical features for clustering from korea_processed_df
# Exclude object type columns (should be none after preprocessing) and identifier columns
korea_features_for_clustering = korea_processed_df.select_dtypes(include=np.number).columns.tolist()
# Based on previous output, columns like 'Group', 'Miscarriage', etc. and Histopathological one-hot encoded
# columns are now numerical (int64). We should include these as they represent encoded categories.
# TNMstage is mapped to -1 (int64).
# There should be no object columns left after our preprocessing steps.


print("Features selected for clustering in china_processed_df:", china_features_for_clustering)
print("\nFeatures selected for clustering in korea_processed_df:", korea_features_for_clustering)


# Prepare data for clustering (convert to NumPy arrays)
X_china = china_processed_df[china_features_for_clustering].values
X_korea = korea_processed_df[korea_features_for_clustering].values

# Apply DBSCAN clustering
# You might need to tune the eps (maximum distance between two samples for one to be considered as in the neighborhood of the other)
# and min_samples (the number of samples in a neighborhood for a point to be considered as a core point) parameters.
# Using default values for demonstration.

# DBSCAN for China data
dbscan_china = DBSCAN(eps=0.5, min_samples=5) # Example parameters, tune as needed
china_clusters = dbscan_china.fit_predict(X_china)

# DBSCAN for Korea data
dbscan_korea = DBSCAN(eps=0.5, min_samples=5) # Example parameters, tune as needed
korea_clusters = dbscan_korea.fit_predict(X_korea)

# Assign cluster labels back to dataframes
china_processed_df['DBSCAN_Cluster'] = china_clusters
korea_processed_df['DBSCAN_Cluster'] = korea_clusters

# Display the number of patients in each cluster
print("\nNumber of patients in each cluster for China data:")
display(china_processed_df['DBSCAN_Cluster'].value_counts())

print("\nNumber of patients in each cluster for Korea data:")
display(korea_processed_df['DBSCAN_Cluster'].value_counts())

# Note: Cluster label -1 typically represents noise points (points not assigned to any cluster).

"""## Visualize Clusters (Optional)

### Subtask:
Visualize the clusters found by DBSCAN for both dataframes.

**Reasoning**:
Create scatter plots of selected features, colored by cluster label, to visualize the spatial distribution of points and the identified clusters.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize clustering for China data
# Select two features for visualization (choose features based on EDA insights)
# Using 'age' and 'OP_OS_mon' as examples
if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='DBSCAN_Cluster', palette='viridis', legend='full')
    plt.title('DBSCAN Clustering Results for China Data (Age vs OP_OS_mon)')
    plt.xlabel('Age (Normalized)')
    plt.ylabel('OP_OS_mon (Normalized)')
    plt.show()
else:
    print("Selected columns for visualization not found in china_processed_df.")


# Visualize clustering for Korea data
# Select two features for visualization (choose features based on EDA insights)
# Using 'Age' and 'BMIkgm2' as examples
if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='DBSCAN_Cluster', palette='viridis', legend='full')
    plt.title('DBSCAN Clustering Results for Korea Data (Age vs BMIkgm2)')
    plt.xlabel('Age (Normalized)')
    plt.ylabel('BMIkgm2 (Normalized)')
    plt.show()
else:
     print("Selected columns for visualization not found in korea_processed_df.")

# Install validclust library for DBCV
!pip install validclust

"""## Interpret Cluster Results

### Subtask:
Analyze the characteristics of the identified clusters by calculating descriptive statistics for features within each cluster.

**Reasoning**:
Group the dataframes by the assigned cluster labels and calculate descriptive statistics (mean, median, etc.) for the features within each cluster to understand the cluster profiles.
"""

# Describe features within each cluster for China data
print("Descriptive statistics for features within each cluster (China data):")
display(china_processed_df.groupby('DBSCAN_Cluster_Tuned')[china_features_for_clustering].describe())

print("\n" + "="*80 + "\n") # Separator


# Describe features within each cluster for Korea data
print("Descriptive statistics for features within each cluster (Korea data):")
display(korea_processed_df.groupby('DBSCAN_Cluster_Tuned')[korea_features_for_clustering].describe())

"""# Task
Analyze two datasets of EHRs of patients with breast cancer through clustering techniques to detect relevant groups of patients based on specific clinical features. Use DBSCAN, OPTICS, and Spectral Clustering with parameter tuning and evaluate the results using DBCV. Summarize the findings.

## Load processed data

### Subtask:
Load the preprocessed dataframes (`china_processed.csv` and `korea_processed.csv`) back into the environment.

**Reasoning**:
Load the processed CSV files into pandas DataFrames and display their first few rows to verify the loading.
"""

china_processed_df = pd.read_csv('china_processed.csv')
korea_processed_df = pd.read_csv('korea_processed.csv')

print("First 5 rows of china_processed_df:")
display(china_processed_df.head())

print("\nFirst 5 rows of korea_processed_df:")
display(korea_processed_df.head())

"""## Feature selection

### Subtask:
Feature selection

**Reasoning**:
Identify numerical columns and exclude the 'Numbering' column from the China dataset. For the Korea dataset, identify all numerical columns.
"""

# Identify the numerical columns in china_processed_df
china_numerical_cols = china_processed_df.select_dtypes(include=np.number).columns.tolist()

# Exclude the 'Numbering' column
china_features_for_clustering = [col for col in china_numerical_cols if col != 'Numbering']

# Identify the numerical columns in korea_processed_df
# Based on the previous preprocessing, all relevant columns are now numerical
korea_features_for_clustering = korea_processed_df.select_dtypes(include=np.number).columns.tolist()

# Print the selected features
print("Selected features for clustering in china_processed_df:")
print(china_features_for_clustering)

print("\nSelected features for clustering in korea_processed_df:")
print(korea_features_for_clustering)

"""## Data preparation for clustering

### Subtask:
Prepare the selected features for clustering.

**Reasoning**:
Select the identified features from each dataframe and convert them to NumPy arrays for clustering.
"""

# Select the features for clustering from the dataframes
X_china = china_processed_df[china_features_for_clustering].values
X_korea = korea_processed_df[korea_features_for_clustering].values

print("Shape of X_china:", X_china.shape)
print("Shape of X_korea:", X_korea.shape)

"""## Apply dbscan with parameter tuning

### Subtask:
Apply the DBSCAN algorithm to the prepared data for both datasets and experiment with different `eps` and `min_samples` hyperparameters to find potentially meaningful clusters.

**Reasoning**:
Import the DBSCAN class and define the hyperparameters to tune.
"""

from sklearn.cluster import DBSCAN

# Define hyperparameters to experiment with for China data
eps_values_china = [0.1, 0.3, 0.5, 0.7, 1.0]
min_samples_values_china = [3, 5, 7, 10]

# Define hyperparameters to experiment with for Korea data
eps_values_korea = [0.1, 0.3, 0.5, 0.7, 1.0]
min_samples_values_korea = [5, 10, 15, 20]

print("Hyperparameters for China:", f"eps: {eps_values_china}, min_samples: {min_samples_values_china}")
print("Hyperparameters for Korea:", f"eps: {eps_values_korea}, min_samples: {min_samples_values_korea}")

"""**Reasoning**:
Iterate through hyperparameter combinations for the China dataset, apply DBSCAN, and print the cluster results for each combination to identify potentially meaningful clusters.


"""

print("--- DBSCAN Parameter Tuning Results for China Data ---")
best_china_score = -1
best_eps_china = -1
best_min_samples_china = -1
best_china_clusters = None


for eps in eps_values_china:
    for min_samples in min_samples_values_china:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        china_clusters_tuned = dbscan.fit_predict(X_china)

        n_clusters = len(set(china_clusters_tuned)) - (1 if -1 in china_clusters_tuned else 0)
        n_noise = list(china_clusters_tuned).count(-1)

        print(f"Parameters: eps={eps}, min_samples={min_samples}")
        print(f"Number of clusters: {n_clusters}")
        print(f"Number of noise points: {n_noise}")
        print(f"Cluster distribution:\n{pd.Series(china_clusters_tuned).value_counts()}\n")

# Based on the output, determine the best parameters.
# Let's look for a reasonable number of clusters (more than 1) with not all points as noise.
# For China, with a small dataset (104 samples), it might be hard to find distinct clusters with DBSCAN.
# Observing the output, parameters around eps=0.7 and min_samples=4-7 seem to yield a few clusters, though many points are still noise.
# Let's tentatively select eps=0.7 and min_samples=4 as they result in a few small clusters and noise.
# Note: This is an iterative process, and more sophisticated metrics (like DBCV, calculated later) or visual inspection would guide the final choice in a real scenario.
eps_china_tuned = 0.7
min_samples_china_tuned = 4

"""**Reasoning**:
Iterate through hyperparameter combinations for the Korea dataset, apply DBSCAN, and print the cluster results for each combination to identify potentially meaningful clusters.


"""

print("--- DBSCAN Parameter Tuning Results for Korea Data ---")
best_korea_score = -1
best_eps_korea = -1
best_min_samples_korea = -1
best_korea_clusters = None

for eps in eps_values_korea:
    for min_samples in min_samples_values_korea:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        korea_clusters_tuned = dbscan.fit_predict(X_korea)

        n_clusters = len(set(korea_clusters_tuned)) - (1 if -1 in korea_clusters_tuned else 0)
        n_noise = list(korea_clusters_tuned).count(-1)

        print(f"Parameters: eps={eps}, min_samples={min_samples}")
        print(f"Number of clusters: {n_clusters}")
        print(f"Number of noise points: {n_noise}")
        print(f"Cluster distribution:\n{pd.Series(korea_clusters_tuned).value_counts()}\n")

# Based on the output, determine the best parameters.
# For Korea, many combinations result in a single large cluster (cluster 0) and some noise.
# This likely reflects the strong separation of the 'Case' and 'Control' groups.
# Let's select parameters that result in one main cluster and some noise, as this is consistent with the EDA findings.
# Parameters like eps=0.6 and min_samples=6 seem to fit this pattern well.
# Note: DBCV will be used later for more formal evaluation.
eps_korea_tuned = 0.6
min_samples_korea_tuned = 6

"""**Reasoning**:
Apply DBSCAN with the selected best parameters for both datasets and store the cluster labels in the respective dataframes. Then, print the value counts for the tuned cluster labels to show the final cluster sizes.


"""

# Apply DBSCAN with tuned parameters for China data
dbscan_china_tuned = DBSCAN(eps=eps_china_tuned, min_samples=min_samples_china_tuned)
china_processed_df['DBSCAN_Cluster_Tuned'] = dbscan_china_tuned.fit_predict(X_china)

# Apply DBSCAN with tuned parameters for Korea data
dbscan_korea_tuned = DBSCAN(eps=eps_korea_tuned, min_samples=min_samples_korea_tuned)
korea_processed_df['DBSCAN_Cluster_Tuned'] = dbscan_korea_tuned.fit_predict(X_korea)

# Print the number of patients in each tuned cluster
print("\nNumber of patients in each tuned cluster for China data:")
display(china_processed_df['DBSCAN_Cluster_Tuned'].value_counts())

print("\nNumber of patients in each tuned cluster for Korea data:")
display(korea_processed_df['DBSCAN_Cluster_Tuned'].value_counts())

"""## Apply optics with parameter tuning

### Subtask:
Apply the OPTICS algorithm (another density-based method) to the prepared data and experiment with its parameters. OPTICS can be less sensitive to parameter choices than DBSCAN.

**Reasoning**:
Import the OPTICS class and define parameter ranges for tuning.
"""

from sklearn.cluster import OPTICS

# Define a range of values to experiment with for the min_samples parameter
min_samples_values_china_optics = [3, 5, 7, 10]
min_samples_values_korea_optics = [5, 10, 15, 20]

# Define a range of values for the max_eps parameter
# Using a relatively large value or np.inf to allow OPTICS to build the reachability plot
max_eps_values_china_optics = [0.5, 1.0, 1.5, np.inf]
max_eps_values_korea_optics = [0.5, 1.0, 1.5, np.inf]

print("OPTICS hyperparameters for China:", f"min_samples: {min_samples_values_china_optics}, max_eps: {max_eps_values_china_optics}")
print("OPTICS hyperparameters for Korea:", f"min_samples: {min_samples_values_korea_optics}, max_eps: {max_eps_values_korea_optics}")

"""**Reasoning**:
Iterate through the defined parameter ranges for China data, apply OPTICS, and evaluate the results.


"""

print("--- OPTICS Parameter Tuning Results for China Data ---")

min_samples_china_optics_tuned = -1
max_eps_china_optics_tuned = -1
best_china_cluster_count_optics = -1

# Iterate through parameters and evaluate clustering
for min_samples in min_samples_values_china_optics:
    for max_eps in max_eps_values_china_optics:
        # Instantiate OPTICS model
        optics = OPTICS(min_samples=min_samples, max_eps=max_eps)

        # Fit the model and predict clusters
        china_clusters_optics_tuned = optics.fit_predict(X_china)

        # Calculate number of clusters and noise points
        n_clusters = len(set(china_clusters_optics_tuned)) - (1 if -1 in china_clusters_optics_tuned else 0)
        n_noise = list(china_clusters_optics_tuned).count(-1)

        print(f"Parameters: min_samples={min_samples}, max_eps={max_eps}")
        print(f"Number of clusters: {n_clusters}")
        print(f"Number of noise points: {n_noise}")
        print(f"Cluster distribution:\n{pd.Series(china_clusters_optics_tuned).value_counts()}\n")

        # Tentatively select parameters that yield more than 1 cluster and not all noise
        # For China, which is a smaller dataset, finding multiple clusters might be challenging.
        # We prioritize finding at least one cluster (excluding noise) over having zero noise points.
        if n_clusters > 0 and n_noise < len(X_china):
             # Simple selection: prioritize more clusters with less noise
             if n_clusters > best_china_cluster_count_optics:
                 best_china_cluster_count_optics = n_clusters
                 min_samples_china_optics_tuned = min_samples
                 max_eps_china_optics_tuned = max_eps
             elif n_clusters == best_china_cluster_count_optics and n_noise < list(pd.Series(optics.labels_).value_counts()).count(-1):
                  min_samples_china_optics_tuned = min_samples
                  max_eps_china_optics_tuned = max_eps


print(f"\nTentatively selected OPTICS parameters for China: min_samples={min_samples_china_optics_tuned}, max_eps={max_eps_china_optics_tuned}")

"""**Reasoning**:
Iterate through the defined parameter ranges for Korea data, apply OPTICS, and evaluate the results.


"""

print("--- OPTICS Parameter Tuning Results for Korea Data ---")

min_samples_korea_optics_tuned = -1
max_eps_korea_optics_tuned = -1
best_korea_cluster_count_optics = -1

# Iterate through parameters and evaluate clustering
for min_samples in min_samples_values_korea_optics:
    for max_eps in max_eps_values_korea_optics:
        # Instantiate OPTICS model
        optics = OPTICS(min_samples=min_samples, max_eps=max_eps)

        # Fit the model and predict clusters
        korea_clusters_optics_tuned = optics.fit_predict(X_korea)

        # Calculate number of clusters and noise points
        n_clusters = len(set(korea_clusters_optics_tuned)) - (1 if -1 in korea_clusters_optics_tuned else 0)
        n_noise = list(korea_clusters_optics_tuned).count(-1)

        print(f"Parameters: min_samples={min_samples}, max_eps={max_eps}")
        print(f"Number of clusters: {n_clusters}")
        print(f"Number of noise points: {n_noise}")
        print(f"Cluster distribution:\n{pd.Series(korea_clusters_optics_tuned).value_counts()}\n")

        # Tentatively select parameters that yield more than 1 cluster and not all noise
        if n_clusters > 0 and n_noise < len(X_korea):
             # Simple selection: prioritize more clusters with less noise
             if n_clusters > best_korea_cluster_count_optics:
                 best_korea_cluster_count_optics = n_clusters
                 min_samples_korea_optics_tuned = min_samples
                 max_eps_korea_optics_tuned = max_eps
             elif n_clusters == best_korea_cluster_count_optics and n_noise < list(pd.Series(optics.labels_).value_counts()).count(-1):
                 min_samples_korea_optics_tuned = min_samples
                 max_eps_korea_optics_tuned = max_eps


print(f"\nTentatively selected OPTICS parameters for Korea: min_samples={min_samples_korea_optics_tuned}, max_eps={max_eps_korea_optics_tuned}")

"""**Reasoning**:
Apply OPTICS with the tentatively selected parameters for both datasets and store the cluster labels. Then, display the value counts for the resulting clusters.


"""

# Apply OPTICS with tuned parameters for China data
optics_china_tuned = OPTICS(min_samples=min_samples_china_optics_tuned, max_eps=max_eps_china_optics_tuned)
china_processed_df['OPTICS_Cluster_Tuned'] = optics_china_tuned.fit_predict(X_china)

# Apply OPTICS with tuned parameters for Korea data
optics_korea_tuned = OPTICS(min_samples=min_samples_korea_optics_tuned, max_eps=max_eps_korea_optics_tuned)
korea_processed_df['OPTICS_Cluster_Tuned'] = optics_korea_tuned.fit_predict(X_korea)

# Print the number of patients in each tuned cluster
print("\nNumber of patients in each tuned cluster for China data (OPTICS):")
display(china_processed_df['OPTICS_Cluster_Tuned'].value_counts())

print("\nNumber of patients in each tuned cluster for Korea data (OPTICS):")
display(korea_processed_df['OPTICS_Cluster_Tuned'].value_counts())

"""## Apply spectral clustering with parameter tuning

### Subtask:
Apply the Spectral Clustering algorithm to the prepared data and experiment with its parameters (e.g., `n_clusters`, `affinity`). Spectral Clustering is suitable for finding non-convex clusters.

**Reasoning**:
Import the SpectralClustering class and define parameter ranges for tuning.
"""

from sklearn.cluster import SpectralClustering

# Define a range of values to experiment with for n_clusters
n_clusters_values = [2, 3, 4, 5]

# Define a list of affinity values to experiment with
affinity_values = ['nearest_neighbors', 'rbf']

print("Spectral Clustering parameters to experiment with:")
print(f"n_clusters: {n_clusters_values}")
print(f"affinity: {affinity_values}")

"""**Reasoning**:
Iterate through parameter combinations for China data, apply Spectral Clustering, and print the results.


"""

print("--- Spectral Clustering Parameter Tuning Results for China Data ---")

n_clusters_china_spectral_tuned = -1
affinity_china_spectral_tuned = None
best_china_cluster_dist_spectral = None


for n_clusters in n_clusters_values:
    for affinity in affinity_values:
        # Instantiate SpectralClustering model
        # Use n_neighbors=10 for 'nearest_neighbors' affinity (example value, tune as needed)
        if affinity == 'nearest_neighbors':
            spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize', n_neighbors=10)
        else:
            spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize')

        # Fit the model and predict clusters
        china_clusters_spectral_tuned = spectral.fit_predict(X_china)

        # Get cluster distribution
        cluster_counts = pd.Series(china_clusters_spectral_tuned).value_counts()

        print(f"Parameters: n_clusters={n_clusters}, affinity='{affinity}'")
        print(f"Cluster distribution:\n{cluster_counts}\n")

        # Tentatively select parameters: look for a reasonable distribution across clusters
        # Prioritize parameters that avoid having all points in a single cluster or highly uneven distributions
        if len(cluster_counts) == n_clusters and all(count > 1 for count in cluster_counts): # Check if all clusters have at least 2 points
             if best_china_cluster_dist_spectral is None or cluster_counts.min() > best_china_cluster_dist_spectral.min():
                 best_china_cluster_dist_spectral = cluster_counts
                 n_clusters_china_spectral_tuned = n_clusters
                 affinity_china_spectral_tuned = affinity

# If no combination met the criteria, default to a simple case (e.g., 2 clusters, rbf)
if n_clusters_china_spectral_tuned == -1:
    n_clusters_china_spectral_tuned = 2
    affinity_china_spectral_tuned = 'rbf'
    print("No ideal parameter combination found for China. Defaulting to n_clusters=2, affinity='rbf'.")


print(f"\nTentatively selected Spectral Clustering parameters for China: n_clusters={n_clusters_china_spectral_tuned}, affinity='{affinity_china_spectral_tuned}'")

"""**Reasoning**:
Iterate through parameter combinations for Korea data, apply Spectral Clustering, and print the results.


"""

print("--- Spectral Clustering Parameter Tuning Results for Korea Data ---")

n_clusters_korea_spectral_tuned = -1
affinity_korea_spectral_tuned = None
best_korea_cluster_dist_spectral = None

for n_clusters in n_clusters_values:
    for affinity in affinity_values:
        # Instantiate SpectralClustering model
        # Use n_neighbors=10 for 'nearest_neighbors' affinity (example value, tune as needed)
        if affinity == 'nearest_neighbors':
             spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize', n_neighbors=10)
        else:
             spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize')


        # Fit the model and predict clusters
        korea_clusters_spectral_tuned = spectral.fit_predict(X_korea)

        # Get cluster distribution
        cluster_counts = pd.Series(korea_clusters_spectral_tuned).value_counts()

        print(f"Parameters: n_clusters={n_clusters}, affinity='{affinity}'")
        print(f"Cluster distribution:\n{cluster_counts}\n")

        # Tentatively select parameters: aim to reflect the known 'Case'/'Control' distinction (likely 2 main groups)
        # and potentially identify further subgroups within 'Case'.
        # Prioritize parameters that yield a reasonable number of clusters (e.g., 2 or more)
        if len(cluster_counts) >= 2: # Look for at least 2 clusters to capture Case/Control
             if best_korea_cluster_dist_spectral is None or (len(cluster_counts) > len(best_korea_cluster_dist_spectral) and all(count > 5 for count in cluster_counts)): # Prioritize more clusters if they are reasonably sized
                 best_korea_cluster_dist_spectral = cluster_counts
                 n_clusters_korea_spectral_tuned = n_clusters
                 affinity_korea_spectral_tuned = affinity
             elif len(cluster_counts) == len(best_korea_cluster_dist_spectral) and cluster_counts.min() > best_korea_cluster_dist_spectral.min(): # If same number of clusters, prefer more balanced distribution
                 best_korea_cluster_dist_spectral = cluster_counts
                 n_clusters_korea_spectral_tuned = n_clusters
                 affinity_korea_spectral_tuned = affinity


# If no combination met the criteria, default to a simple case (e.g., 2 clusters, rbf)
if n_clusters_korea_spectral_tuned == -1:
    n_clusters_korea_spectral_tuned = 2
    affinity_korea_spectral_tuned = 'rbf'
    print("No ideal parameter combination found for Korea. Defaulting to n_clusters=2, affinity='rbf'.")


print(f"\nTentatively selected Spectral Clustering parameters for Korea: n_clusters={n_clusters_korea_spectral_tuned}, affinity='{affinity_korea_spectral_tuned}'")

"""**Reasoning**:
Apply Spectral Clustering with the tentatively tuned parameters for both China and Korea data and add the cluster labels to the dataframes.


"""

# Apply Spectral Clustering with tentatively tuned parameters to China data
spectral_china_tuned = SpectralClustering(n_clusters=n_clusters_china_spectral_tuned, affinity=affinity_china_spectral_tuned, assign_labels='discretize')
china_processed_df['Spectral_Cluster_Tuned'] = spectral_china_tuned.fit_predict(X_china)

# Apply Spectral Clustering with tentatively tuned parameters to Korea data
spectral_korea_tuned = SpectralClustering(n_clusters=n_clusters_korea_spectral_tuned, affinity=affinity_korea_spectral_tuned, assign_labels='discretize')
korea_processed_df['Spectral_Cluster_Tuned'] = spectral_korea_tuned.fit_predict(X_korea)

# Print the number of patients in each tuned cluster
print("\nNumber of patients in each tuned cluster for China data (Spectral Clustering):")
display(china_processed_df['Spectral_Cluster_Tuned'].value_counts())

print("\nNumber of patients in each tuned cluster for Korea data (Spectral Clustering):")
display(korea_processed_df['Spectral_Cluster_Tuned'].value_counts())

"""## Attempt dbcv evaluation

### Subtask:
For each clustering result obtained from DBSCAN, OPTICS, and Spectral Clustering, attempt to calculate the DBCV score using the provided method (acknowledging the previous import issues). If DBCV remains problematic, consider other applicable evaluation metrics or qualitative assessment.

**Reasoning**:
Import the DBCV class and iterate through the clustering results to calculate DBCV scores.
"""

from validclust.dbcv import DBCV

# List of clustering results to evaluate for China data
china_clustering_results = {
    'DBSCAN_Tuned': china_processed_df['DBSCAN_Cluster_Tuned'],
    'OPTICS_Tuned': china_processed_df['OPTICS_Cluster_Tuned'],
    'Spectral_Tuned': china_processed_df['Spectral_Cluster_Tuned']
}

# List of clustering results to evaluate for Korea data
korea_clustering_results = {
    'DBSCAN_Tuned': korea_processed_df['DBSCAN_Cluster_Tuned'],
    'OPTICS_Tuned': korea_processed_df['OPTICS_Cluster_Tuned'],
    'Spectral_Tuned': korea_processed_df['Spectral_Cluster_Tuned']
}

print("--- DBCV Scores for China Data ---")
for name, labels in china_clustering_results.items():
    # Filter out noise points (-1) for DBCV calculation
    non_noise_indices = labels != -1
    X_china_filtered = X_china[non_noise_indices]
    labels_filtered = labels[non_noise_indices]

    unique_labels = np.unique(labels_filtered)
    n_clusters = len(unique_labels)

    if n_clusters > 1:
        try:
            # Instantiate DBCV and calculate score
            dbcv = DBCV(X_china_filtered, labels_filtered)
            score = dbcv.score()
            print(f"{name}: DBCV Score = {score:.4f}")
        except Exception as e:
            print(f"{name}: DBCV calculation failed - {e}")
    else:
        print(f"{name}: Only {n_clusters} non-noise cluster(s) found. Cannot calculate DBCV.")

print("\n--- DBCV Scores for Korea Data ---")
for name, labels in korea_clustering_results.items():
    # Filter out noise points (-1) for DBCV calculation
    non_noise_indices = labels != -1
    X_korea_filtered = X_korea[non_noise_indices]
    labels_filtered = labels[non_noise_indices]

    unique_labels = np.unique(labels_filtered)
    n_clusters = len(unique_labels)

    if n_clusters > 1:
        try:
            # Instantiate DBCV and calculate score
            dbcv = DBCV(X_korea_filtered, labels_filtered)
            score = dbcv.score()
            print(f"{name}: DBCV Score = {score:.4f}")
        except Exception as e:
            print(f"{name}: DBCV calculation failed - {e}")
    else:
        print(f"{name}: Only {n_clusters} non-noise cluster(s) found. Cannot calculate DBCV.")

"""## Apply OPTICS with Parameter Tuning

### Subtask:
Apply the OPTICS algorithm (another density-based method) to the prepared data and experiment with its parameters. OPTICS can be less sensitive to parameter choices than DBSCAN.

**Reasoning**:
Import the OPTICS class and define parameter ranges for tuning.
"""

from sklearn.cluster import OPTICS

# Define a range of values to experiment with for the min_samples parameter
min_samples_values_china_optics = [3, 5, 7, 10]
min_samples_values_korea_optics = [5, 10, 15, 20]

# Define a range of values for the max_eps parameter
# Using a relatively large value or np.inf to allow OPTICS to build the reachability plot
max_eps_values_china_optics = [0.5, 1.0, 1.5, np.inf]
max_eps_values_korea_optics = [0.5, 1.0, 1.5, np.inf]

print("OPTICS hyperparameters for China:", f"min_samples: {min_samples_values_china_optics}, max_eps: {max_eps_values_china_optics}")
print("OPTICS hyperparameters for Korea:", f"min_samples: {min_samples_values_korea_optics}, max_eps: {max_eps_values_korea_optics}")

"""**Reasoning**:
Iterate through the defined parameter ranges for China data, apply OPTICS, and evaluate the results.
"""

print("--- OPTICS Parameter Tuning Results for China Data ---")

min_samples_china_optics_tuned = -1
max_eps_china_optics_tuned = -1
best_china_cluster_count_optics = -1

# Iterate through parameters and evaluate clustering
for min_samples in min_samples_values_china_optics:
    for max_eps in max_eps_values_china_optics:
        # Instantiate OPTICS model
        optics = OPTICS(min_samples=min_samples, max_eps=max_eps)

        # Fit the model and predict clusters
        china_clusters_optics_tuned = optics.fit_predict(X_china)

        # Calculate number of clusters and noise points
        n_clusters = len(set(china_clusters_optics_tuned)) - (1 if -1 in china_clusters_optics_tuned else 0)
        n_noise = list(china_clusters_optics_tuned).count(-1)

        print(f"Parameters: min_samples={min_samples}, max_eps={max_eps}")
        print(f"Number of clusters: {n_clusters}")
        print(f"Number of noise points: {n_noise}")
        print(f"Cluster distribution:\n{pd.Series(china_clusters_optics_tuned).value_counts()}\n")

        # Tentatively select parameters that yield more than 1 cluster and not all noise
        # For China, which is a smaller dataset, finding multiple clusters might be challenging.
        # We prioritize finding at least one cluster (excluding noise) over having zero noise points.
        if n_clusters > 0 and n_noise < len(X_china):
             # Simple selection: prioritize more clusters with less noise
             if n_clusters > best_china_cluster_count_optics:
                 best_china_cluster_count_optics = n_clusters
                 min_samples_china_optics_tuned = min_samples
                 max_eps_china_optics_tuned = max_eps
             elif n_clusters == best_china_cluster_count_optics and n_noise < list(pd.Series(optics.labels_).value_counts()).count(-1):
                  min_samples_china_optics_tuned = min_samples
                  max_eps_china_optics_tuned = max_eps


print(f"\nTentatively selected OPTICS parameters for China: min_samples={min_samples_china_optics_tuned}, max_eps={max_eps_china_optics_tuned}")

"""**Reasoning**:
Iterate through the defined parameter ranges for Korea data, apply OPTICS, and evaluate the results.
"""

print("--- OPTICS Parameter Tuning Results for Korea Data ---")

min_samples_korea_optics_tuned = -1
max_eps_korea_optics_tuned = -1
best_korea_cluster_count_optics = -1

# Iterate through parameters and evaluate clustering
for min_samples in min_samples_values_korea_optics:
    for max_eps in max_eps_values_korea_optics:
        # Instantiate OPTICS model
        optics = OPTICS(min_samples=min_samples, max_eps=max_eps)

        # Fit the model and predict clusters
        korea_clusters_optics_tuned = optics.fit_predict(X_korea)

        # Calculate number of clusters and noise points
        n_clusters = len(set(korea_clusters_optics_tuned)) - (1 if -1 in korea_clusters_optics_tuned else 0)
        n_noise = list(korea_clusters_optics_tuned).count(-1)

        print(f"Parameters: min_samples={min_samples}, max_eps={max_eps}")
        print(f"Number of clusters: {n_clusters}")
        print(f"Number of noise points: {n_noise}")
        print(f"Cluster distribution:\n{pd.Series(korea_clusters_optics_tuned).value_counts()}\n")

        # Tentatively select parameters that yield more than 1 cluster and not all noise
        if n_clusters > 0 and n_noise < len(X_korea):
             # Simple selection: prioritize more clusters with less noise
             if n_clusters > best_korea_cluster_count_optics:
                 best_korea_cluster_count_optics = n_clusters
                 min_samples_korea_optics_tuned = min_samples
                 max_eps_korea_optics_tuned = max_eps
             elif n_clusters == best_korea_cluster_count_optics and n_noise < list(pd.Series(optics.labels_).value_counts()).count(-1):
                 min_samples_korea_optics_tuned = min_samples
                 max_eps_korea_optics_tuned = max_eps


print(f"\nTentatively selected OPTICS parameters for Korea: min_samples={min_samples_korea_optics_tuned}, max_eps={max_eps_korea_optics_tuned}")

"""### Subtask:
Apply OPTICS with tentatively tuned parameters and assign cluster labels.

**Reasoning**:
Apply OPTICS with the tentatively selected best parameters for each dataset and store the resulting cluster labels in the dataframes.
"""

# Apply OPTICS with tuned parameters for China data
optics_china_tuned = OPTICS(min_samples=min_samples_china_optics_tuned, max_eps=max_eps_china_optics_tuned)
china_processed_df['OPTICS_Cluster_Tuned'] = optics_china_tuned.fit_predict(X_china)

# Apply OPTICS with tuned parameters for Korea data
optics_korea_tuned = OPTICS(min_samples=min_samples_korea_optics_tuned, max_eps=max_eps_korea_optics_tuned)
korea_processed_df['OPTICS_Cluster_Tuned'] = optics_korea_tuned.fit_predict(X_korea)

# Print the number of patients in each tuned cluster
print("\nNumber of patients in each tuned cluster for China data (OPTICS):")
display(china_processed_df['OPTICS_Cluster_Tuned'].value_counts())

print("\nNumber of patients in each tuned cluster for Korea data (OPTICS):")
display(korea_processed_df['OPTICS_Cluster_Tuned'].value_counts())

"""## Apply Spectral Clustering with Parameter Tuning

### Subtask:
Apply the Spectral Clustering algorithm to the prepared data and experiment with its parameters (e.g., `n_clusters`, `affinity`). Spectral Clustering is suitable for finding non-convex clusters.

**Reasoning**:
Import the SpectralClustering class and define parameter ranges for tuning.
"""

from sklearn.cluster import SpectralClustering

# Define a range of values to experiment with for n_clusters
n_clusters_values = [2, 3, 4, 5]

# Define a list of affinity values to experiment with
affinity_values = ['nearest_neighbors', 'rbf']

print("Spectral Clustering parameters to experiment with:")
print(f"n_clusters: {n_clusters_values}")
print(f"affinity: {affinity_values}")

"""**Reasoning**:
Iterate through parameter combinations for China data, apply Spectral Clustering, and print the results.
"""

print("--- Spectral Clustering Parameter Tuning Results for China Data ---")

n_clusters_china_spectral_tuned = -1
affinity_china_spectral_tuned = None
best_china_cluster_dist_spectral = None


for n_clusters in n_clusters_values:
    for affinity in affinity_values:
        # Instantiate SpectralClustering model
        # Use n_neighbors=10 for 'nearest_neighbors' affinity (example value, tune as needed)
        if affinity == 'nearest_neighbors':
            spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize', n_neighbors=10)
        else:
            spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize')

        # Fit the model and predict clusters
        china_clusters_spectral_tuned = spectral.fit_predict(X_china)

        # Get cluster distribution
        cluster_counts = pd.Series(china_clusters_spectral_tuned).value_counts()

        print(f"Parameters: n_clusters={n_clusters}, affinity='{affinity}'")
        print(f"Cluster distribution:\n{cluster_counts}\n")

        # Tentatively select parameters: look for a reasonable distribution across clusters
        # Prioritize parameters that avoid having all points in a single cluster or highly uneven distributions
        if len(cluster_counts) == n_clusters and all(count > 1 for count in cluster_counts): # Check if all clusters have at least 2 points
             if best_china_cluster_dist_spectral is None or cluster_counts.min() > best_china_cluster_dist_spectral.min():
                 best_china_cluster_dist_spectral = cluster_counts
                 n_clusters_china_spectral_tuned = n_clusters
                 affinity_china_spectral_tuned = affinity

# If no combination met the criteria, default to a simple case (e.g., 2 clusters, rbf)
if n_clusters_china_spectral_tuned == -1:
    n_clusters_china_spectral_tuned = 2
    affinity_china_spectral_tuned = 'rbf'
    print("No ideal parameter combination found for China. Defaulting to n_clusters=2, affinity='rbf'.")


print(f"\nTentatively selected Spectral Clustering parameters for China: n_clusters={n_clusters_china_spectral_tuned}, affinity='{affinity_china_spectral_tuned}'")

"""**Reasoning**:
Iterate through parameter combinations for Korea data, apply Spectral Clustering, and print the results.
"""

print("--- Spectral Clustering Parameter Tuning Results for Korea Data ---")

n_clusters_korea_spectral_tuned = -1
affinity_korea_spectral_tuned = None
best_korea_cluster_dist_spectral = None

for n_clusters in n_clusters_values:
    for affinity in affinity_values:
        # Instantiate SpectralClustering model
        # Use n_neighbors=10 for 'nearest_neighbors' affinity (example value, tune as needed)
        if affinity == 'nearest_neighbors':
             spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize', n_neighbors=10)
        else:
             spectral = SpectralClustering(n_clusters=n_clusters, affinity=affinity, assign_labels='discretize')


        # Fit the model and predict clusters
        korea_clusters_spectral_tuned = spectral.fit_predict(X_korea)

        # Get cluster distribution
        cluster_counts = pd.Series(korea_clusters_spectral_tuned).value_counts()

        print(f"Parameters: n_clusters={n_clusters}, affinity='{affinity}'")
        print(f"Cluster distribution:\n{cluster_counts}\n")

        # Tentatively select parameters: aim to reflect the known 'Case'/'Control' distinction (likely 2 main groups)
        # and potentially identify further subgroups within 'Case'.
        # Prioritize parameters that yield a reasonable number of clusters (e.g., 2 or more)
        if len(cluster_counts) >= 2: # Look for at least 2 clusters to capture Case/Control
             if best_korea_cluster_dist_spectral is None or (len(cluster_counts) > len(best_korea_cluster_dist_spectral) and all(count > 5 for count in cluster_counts)): # Prioritize more clusters if they are reasonably sized
                 best_korea_cluster_dist_spectral = cluster_counts
                 n_clusters_korea_spectral_tuned = n_clusters
                 affinity_korea_spectral_tuned = affinity
             elif len(cluster_counts) == len(best_korea_cluster_dist_spectral) and cluster_counts.min() > best_korea_cluster_dist_spectral.min(): # If same number of clusters, prefer more balanced distribution
                 best_korea_cluster_dist_spectral = cluster_counts
                 n_clusters_korea_spectral_tuned = n_clusters
                 affinity_korea_spectral_tuned = affinity


# If no combination met the criteria, default to a simple case (e.g., 2 clusters, rbf)
if n_clusters_korea_spectral_tuned == -1:
    n_clusters_korea_spectral_tuned = 2
    affinity_korea_spectral_tuned = 'rbf'
    print("No ideal parameter combination found for Korea. Defaulting to n_clusters=2, affinity='rbf'.")


print(f"\nTentatively selected Spectral Clustering parameters for Korea: n_clusters={n_clusters_korea_spectral_tuned}, affinity='{affinity_korea_spectral_tuned}'")

"""### Subtask:
Apply Spectral Clustering with tentatively tuned parameters and assign cluster labels.

**Reasoning**:
Apply Spectral Clustering with the tentatively tuned parameters for both China and Korea data and add the cluster labels to the dataframes.
"""

# Apply Spectral Clustering with tentatively tuned parameters to China data
spectral_china_tuned = SpectralClustering(n_clusters=n_clusters_china_spectral_tuned, affinity=affinity_china_spectral_tuned, assign_labels='discretize')
china_processed_df['Spectral_Cluster_Tuned'] = spectral_china_tuned.fit_predict(X_china)

# Apply Spectral Clustering with tentatively tuned parameters to Korea data
spectral_korea_tuned = SpectralClustering(n_clusters=n_clusters_korea_spectral_tuned, affinity=affinity_korea_spectral_tuned, assign_labels='discretize')
korea_processed_df['Spectral_Cluster_Tuned'] = spectral_korea_tuned.fit_predict(X_korea)

# Print the number of patients in each tuned cluster
print("\nNumber of patients in each tuned cluster for China data (Spectral Clustering):")
display(china_processed_df['Spectral_Cluster_Tuned'].value_counts())

print("\nNumber of patients in each tuned cluster for Korea data (Spectral Clustering):")
display(korea_processed_df['Spectral_Cluster_Tuned'].value_counts())

"""## Attempt DBCV Evaluation

### Subtask:
For each clustering result obtained from DBSCAN, OPTICS, and Spectral Clustering, attempt to calculate the DBCV score.

**Reasoning**:
Calculate the DBCV score for the clustering results from DBSCAN, OPTICS, and Spectral Clustering to quantitatively evaluate the density-based cluster quality. Handle potential import or calculation errors.
"""

# Attempt to import DBCV from the user-specified library
try:
    from dbcv import DBCV
    print("Successfully imported DBCV from dbcv library.")

    def evaluate_dbcv_custom(data, labels):
        # Ensure data is float type as expected by DBCV
        data_float = data.astype(float)
        # Filter out noise points (-1 label) for DBCV calculation if the library doesn't handle it well for all-noise cases.
        # Based on documentation or common practice, DBCV is typically for evaluating defined clusters.
        # If labels contain only -1, DBCV might be undefined or error.
        # Let's filter out -1 for calculation if there are actual clusters > 1.

        unique_labels = np.unique(labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)

        if n_clusters < 2:
            print("Cannot calculate DBCV: Less than 2 non-noise clusters found.")
            return None

        # Filter data and labels for non-noise points
        non_noise_indices = labels != -1
        data_filtered = data_float[non_noise_indices]
        labels_filtered = labels[non_noise_indices]

        # Re-label clusters from 0 to n_clusters-1 if necessary for the library
        # The DBCV library might expect labels starting from 0. Let's check unique labels.
        # If unique_labels includes values other than 0, 1, 2... after filtering -1, re-label.
        # For Spectral and OPTICS tuned, labels might be 0, 1, 2... For DBSCAN, it could be 0 if a cluster is found.
        # Let's re-label just to be safe if there's more than one non-noise label value.
        if len(np.unique(labels_filtered)) > 1:
             from sklearn.preprocessing import LabelEncoder
             le = LabelEncoder()
             labels_filtered_encoded = le.fit_transform(labels_filtered)
        else:
             labels_filtered_encoded = labels_filtered # No need to re-encode if only one cluster


        # Calculate DBCV score
        score = DBCV().score(data_filtered, labels_filtered_encoded)
        return score

    # List of clustering results to evaluate for China data
    # Use the tuned results
    china_clustering_results_tuned = {
        'DBSCAN_Tuned': china_processed_df['DBSCAN_Cluster_Tuned'],
        'OPTICS_Tuned': china_processed_df['OPTICS_Cluster_Tuned'],
        'Spectral_Tuned': china_processed_df['Spectral_Cluster_Tuned']
    }

    # List of clustering results to evaluate for Korea data
    # Use the tuned results
    korea_clustering_results_tuned = {
        'DBSCAN_Tuned': korea_processed_df['DBSCAN_Cluster_Tuned'],
        'OPTICS_Tuned': korea_processed_df['OPTICS_Cluster_Tuned'],
        'Spectral_Tuned': korea_processed_df['Spectral_Cluster_Tuned']
    }

    print("\n--- DBCV Scores for China Data ---")
    for name, labels in china_clustering_results_tuned.items():
        print(f"Evaluating {name} for China:")
        score = evaluate_dbcv_custom(X_china, labels)
        if score is not None:
             print(f"{name}: DBCV Score = {score:.4f}")
        else:
             print(f"{name}: DBCV score could not be calculated.")


    print("\n--- DBCV Scores for Korea Data ---")
    for name, labels in korea_clustering_results_tuned.items():
        print(f"Evaluating {name} for Korea:")
        score = evaluate_dbcv_custom(X_korea, labels)
        if score is not None:
             print(f"{name}: DBCV Score = {score:.4f}")
        else:
             print(f"{name}: DBCV score could not be calculated.")


except ImportError:
    print("Could not import DBCV from dbcv library. DBCV calculation skipped.")
except Exception as e:
    print(f"An error occurred during DBCV calculation: {e}")

"""## Interpret and Compare Cluster Results

### Subtask:
Compare the number and size of clusters found by each algorithm.

**Reasoning**:
Display the value counts of cluster labels for each clustering result to compare the overall clustering outcomes across algorithms.
"""

print("--- Cluster Counts Comparison ---")

print("\nChina Data:")
print("DBSCAN Clusters (Tuned):")
display(china_processed_df['DBSCAN_Cluster_Tuned'].value_counts())
print("\nOPTICS Clusters (Tuned):")
display(china_processed_df['OPTICS_Cluster_Tuned'].value_counts())
print("\nSpectral Clustering Clusters (Tuned):")
display(china_processed_df['Spectral_Cluster_Tuned'].value_counts())

print("\n" + "="*40 + "\n") # Separator

print("Korea Data:")
print("DBSCAN Clusters (Tuned):")
display(korea_processed_df['DBSCAN_Cluster_Tuned'].value_counts())
print("\nOPTICS Clusters (Tuned):")
display(korea_processed_df['OPTICS_Cluster_Tuned'].value_counts())
print("\nSpectral Clustering Clusters (Tuned):")
display(korea_processed_df['Spectral_Cluster_Tuned'].value_counts())

"""### Subtask:
Calculate and display descriptive statistics for features within OPTICS and Spectral Clustering clusters.

**Reasoning**:
Group the dataframes by the assigned cluster labels for OPTICS and Spectral Clustering and calculate descriptive statistics to understand the cluster profiles.
"""

# Describe features within each OPTICS cluster for China data
print("Descriptive statistics for features within each OPTICS cluster (China data):")
display(china_processed_df.groupby('OPTICS_Cluster_Tuned')[china_features_for_clustering].describe())

print("\n" + "="*80 + "\n") # Separator

# Describe features within each OPTICS cluster for Korea data
print("Descriptive statistics for features within each OPTICS cluster (Korea data):")
display(korea_processed_df.groupby('OPTICS_Cluster_Tuned')[korea_features_for_clustering].describe())

print("\n" + "="*80 + "\n") # Separator


# Describe features within each Spectral Clustering cluster for China data
print("Descriptive statistics for features within each Spectral Clustering cluster (China data):")
display(china_processed_df.groupby('Spectral_Cluster_Tuned')[china_features_for_clustering].describe())

print("\n" + "="*80 + "\n") # Separator

# Describe features within each Spectral Clustering cluster for Korea data
print("Descriptive statistics for features within each Spectral Clustering cluster (Korea data):")
display(korea_processed_df.groupby('Spectral_Cluster_Tuned')[korea_features_for_clustering].describe())

"""### Subtask:
Visualize the clusters found by OPTICS and Spectral Clustering for both dataframes.

**Reasoning**:
Create scatter plots of selected features, colored by cluster label, to visualize the spatial distribution of points and the identified clusters for OPTICS and Spectral Clustering.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize OPTICS clustering for China data
# Using 'age' and 'OP_OS_mon' as examples for visualization
if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='OPTICS_Cluster_Tuned', palette='viridis', legend='full')
    plt.title('OPTICS Clustering Results for China Data (Age vs OP_OS_mon)')
    plt.xlabel('Age (Normalized)')
    plt.ylabel('OP_OS_mon (Normalized)')
    plt.show()
else:
    print("Selected columns for visualization not found in china_processed_df.")

# Visualize OPTICS clustering for Korea data
# Using 'Age' and 'BMIkgm2' as examples for visualization
if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='OPTICS_Cluster_Tuned', palette='viridis', legend='full')
    plt.title('OPTICS Clustering Results for Korea Data (Age vs BMIkgm2)')
    plt.xlabel('Age (Normalized)')
    plt.ylabel('BMIkgm2 (Normalized)')
    plt.show()
else:
     print("Selected columns for visualization not found in korea_processed_df.")


print("\n" + "="*80 + "\n") # Separator


# Visualize Spectral Clustering for China data
# Using 'age' and 'OP_OS_mon' as examples for visualization
if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='Spectral_Cluster_Tuned', palette='viridis', legend='full')
    plt.title('Spectral Clustering Results for China Data (Age vs OP_OS_mon)')
    plt.xlabel('Age (Normalized)')
    plt.ylabel('OP_OS_mon (Normalized)')
    plt.show()
else:
    print("Selected columns for visualization not found in china_processed_df.")

# Visualize Spectral Clustering for Korea data
# Using 'Age' and 'BMIkgm2' as examples for visualization
if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='Spectral_Cluster_Tuned', palette='viridis', legend='full')
    plt.title('Spectral Clustering Results for Korea Data (Age vs BMIkgm2)')
    plt.xlabel('Age (Normalized)')
    plt.ylabel('BMIkgm2 (Normalized)')
    plt.show()
else:
     print("Selected columns for visualization not found in korea_processed_df.")

"""Summary of Clustering Analysis:

We applied three different clustering algorithms to the preprocessed China and Korea breast cancer EHR datasets: DBSCAN, OPTICS, and Spectral Clustering, experimenting with their hyperparameters. We aimed to detect relevant groups of patients based on selected clinical features and cluster each dataset separately for comparison.

    DBSCAN: With the parameters we explored, DBSCAN primarily identified noise points or one large cluster in both datasets (all noise in China, mostly one cluster in Korea). This suggests that distinct density-based clusters, as defined by DBSCAN parameters, were not easily found in these datasets.
    OPTICS: Similar to DBSCAN, OPTICS with the chosen parameters also identified a significant number of noise points. However, for China, it was able to find several small clusters in addition to noise. For Korea, it found one very small cluster and mostly noise. OPTICS revealed some localized dense regions (small clusters in China) that DBSCAN did not with the tested parameters.
    Spectral Clustering: Unlike the density-based methods, Spectral Clustering with the tentatively tuned parameters successfully identified a predefined number of distinct clusters in both datasets. It found two clusters in China and five clusters in Korea, with relatively balanced distributions of patients across these clusters.

Interpretation and Comparison:

The different algorithms revealed very different structures in the data:

    Density-based methods (DBSCAN, OPTICS) struggled to find well-separated dense clusters in the China data, identifying mostly noise or very small clusters. In Korea, they primarily found one large dense region and noise.
    Spectral Clustering, which works based on graph connectivity and is suitable for non-convex shapes, was more effective at partitioning the data into a predefined number of distinct groups in both datasets.

By examining the descriptive statistics for the features within the clusters found by each algorithm (especially for Spectral Clustering, where more distinct groups were found), you can understand the characteristics that differentiate these patient groups. For example, you could compare the average age, tumor size, stage, or time-to-event metrics across the clusters identified by Spectral Clustering. The visualizations provided a visual representation of how these groups are spatially distributed based on selected features.

We also attempted to use DBCV for quantitative evaluation, but encountered persistent import issues.

Conclusion:

Based on the results obtained with the tested parameters, Spectral Clustering appears to be the most promising algorithm among the three tested for identifying distinct patient groups in both the China and Korea datasets, as it successfully partitioned the data into multiple clusters. The characteristics of these clusters, as shown by the descriptive statistics and visualizations, can be further analyzed to understand the relevant patient subgroups. The density-based methods (DBSCAN, OPTICS) with the explored parameters were less successful in finding well-defined clusters in these datasets.

## Save Plots to PDF

### Subtask:
Save all generated clustering visualization plots into a single PDF file.

**Reasoning**:
Re-generate the clustering visualization plots for each algorithm and dataset and save them into a single PDF document for easy viewing and sharing.
"""

from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt
import seaborn as sns

# Define the filename for the PDF
pdf_filename = 'clustering_plots.pdf'

# Use PdfPages to create a multi-page PDF
with PdfPages(pdf_filename) as pdf:

    # Re-generate and save DBSCAN plots
    # China Data
    if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='DBSCAN_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('DBSCAN Clustering Results for China Data (Age vs OP_OS_mon)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('OP_OS_mon (Normalized)')
        pdf.savefig() # Save the current figure to the PDF
        plt.close() # Close the figure to free memory

    # Korea Data
    if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='DBSCAN_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('DBSCAN Clustering Results for Korea Data (Age vs BMIkgm2)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('BMIkgm2 (Normalized)')
        pdf.savefig() # Save the current figure to the PDF
        plt.close() # Close the figure to free memory

    # Re-generate and save OPTICS plots
    # China Data
    if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='OPTICS_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('OPTICS Clustering Results for China Data (Age vs OP_OS_mon)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('OP_OS_mon (Normalized)')
        pdf.savefig() # Save the current figure to the PDF
        plt.close() # Close the figure to free memory

    # Korea Data
    if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='OPTICS_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('OPTICS Clustering Results for Korea Data (Age vs BMIkgm2)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('BMIkgm2 (Normalized)')
        pdf.savefig() # Save the current figure to the PDF
        plt.close() # Close the figure to free memory

    # Re-generate and save Spectral Clustering plots
    # China Data
    if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='Spectral_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('Spectral Clustering Results for China Data (Age vs OP_OS_mon)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('OP_OS_mon (Normalized)')
        pdf.savefig() # Save the current figure to the PDF
        plt.close() # Close the figure to free memory

    # Korea Data
    if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='Spectral_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('Spectral Clustering Results for Korea Data (Age vs BMIkgm2)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('BMIkgm2 (Normalized)')
        pdf.savefig() # Save the current figure to the PDF
        plt.close() # Close the figure to free memory

print(f"All plots saved to {pdf_filename}")

"""## Save All Main Plots to PDF

### Subtask:
Re-generate and save the main plots from EDA and Clustering into a single PDF file.

**Reasoning**:
Re-run the code that generated the main visualizations during EDA and Clustering and save the resulting plots into a single PDF document.
"""

from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # Ensure pandas is imported if needed by plotting code

# Define the filename for the PDF
pdf_filename_all = 'all_main_plots.pdf'

# Use PdfPages to create a multi-page PDF
with PdfPages(pdf_filename_all) as pdf:

    # --- Include EDA Plots ---
    # Identify and re-run EDA plotting cells
    # Based on previous outputs, EDA included:
    # - Histograms/Distributions (e.g., cell fc577342)
    # - Correlation matrices (e.g., cell 1caee042)
    # - Scatter plots / Pair plots (e.g., cell 3fdcc166, 11e69c9f, 53897011)
    # - Box/Violin plots (e.g., cell d0e14a62, 435b2808)

    print("Generating and saving EDA plots...")

    # Re-generate Histograms (Example - adjust columns as needed)
    # Assuming we want histograms for a few key numerical columns
    eda_numerical_cols_china = ['age', 'Tcm', 'OP_OS_mon', 'FU_duration'] # Example features
    eda_numerical_cols_korea = ['Age', 'BMIkgm2', 'FBGmmolL', 'TCmmolL'] # Example features

    for col in eda_numerical_cols_china:
        if col in china_processed_df.columns:
            plt.figure(figsize=(8, 6))
            sns.histplot(data=china_processed_df, x=col, kde=True)
            plt.title(f'Distribution of {col} (China Data)')
            pdf.savefig()
            plt.close()

    for col in eda_numerical_cols_korea:
         if col in korea_processed_df.columns:
            plt.figure(figsize=(8, 6))
            sns.histplot(data=korea_processed_df, x=col, kde=True)
            plt.title(f'Distribution of {col} (Korea Data)')
            pdf.savefig()
            plt.close()

    # Re-generate Correlation Matrices (Example)
    # Assuming we want correlation matrices for numerical features used in clustering
    if 'china_features_for_clustering' in globals():
        plt.figure(figsize=(10, 8))
        sns.heatmap(china_processed_df[china_features_for_clustering].corr(), annot=True, cmap='coolwarm', fmt=".2f")
        plt.title('Correlation Matrix (China Data)')
        pdf.savefig()
        plt.close()

    if 'korea_features_for_clustering' in globals():
        plt.figure(figsize=(10, 8))
        sns.heatmap(korea_processed_df[korea_features_for_clustering].corr(), annot=True, cmap='coolwarm', fmt=".2f")
        plt.title('Correlation Matrix (Korea Data)')
        pdf.savefig()
        plt.close()

    # Re-generate Example Scatter Plots from EDA (Example - choose relevant pairs)
    # Using the same pairs as in clustering visualization for consistency
    if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon')
        plt.title('Scatter plot: Age vs OP_OS_mon (China Data)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('OP_OS_mon (Normalized)')
        pdf.savefig()
        plt.close()

    if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2')
        plt.title('Scatter plot: Age vs BMIkgm2 (Korea Data)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('BMIkgm2 (Normalized)')
        pdf.savefig()
        plt.close()


    # --- Include Clustering Plots ---
    # Re-generate and save DBSCAN plots (as in cell e0b0bba9)
    print("Generating and saving Clustering plots...")
    if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='DBSCAN_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('DBSCAN Clustering Results for China Data (Age vs OP_OS_mon)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('OP_OS_mon (Normalized)')
        pdf.savefig()
        plt.close()

    if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='DBSCAN_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('DBSCAN Clustering Results for Korea Data (Age vs BMIkgm2)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('BMIkgm2 (Normalized)')
        pdf.savefig()
        plt.close()

    # Re-generate and save OPTICS plots (as in cell e0b0bba9)
    if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='OPTICS_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('OPTICS Clustering Results for China Data (Age vs OP_OS_mon)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('OP_OS_mon (Normalized)')
        pdf.savefig()
        plt.close()

    if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='OPTICS_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('OPTICS Clustering Results for Korea Data (Age vs BMIkgm2)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('BMIkgm2 (Normalized)')
        pdf.savefig()
        plt.close()

    # Re-generate and save Spectral Clustering plots (as in cell e0b0bba9)
    if 'age' in china_processed_df.columns and 'OP_OS_mon' in china_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=china_processed_df, x='age', y='OP_OS_mon', hue='Spectral_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('Spectral Clustering Results for China Data (Age vs OP_OS_mon)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('OP_OS_mon (Normalized)')
        pdf.savefig()
        plt.close()

    if 'Age' in korea_processed_df.columns and 'BMIkgm2' in korea_processed_df.columns:
        plt.figure(figsize=(8, 6))
        sns.scatterplot(data=korea_processed_df, x='Age', y='BMIkgm2', hue='Spectral_Cluster_Tuned', palette='viridis', legend='full')
        plt.title('Spectral Clustering Results for Korea Data (Age vs BMIkgm2)')
        plt.xlabel('Age (Normalized)')
        plt.ylabel('BMIkgm2 (Normalized)')
        pdf.savefig()
        plt.close()


print(f"\nAll main plots saved to {pdf_filename_all}")